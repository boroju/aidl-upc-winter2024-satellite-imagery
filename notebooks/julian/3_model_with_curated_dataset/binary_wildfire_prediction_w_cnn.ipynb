{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"15NATz0q-fSQ4IxodZYYuZxvItU-yLTUx","authorship_tag":"ABX9TyOeBwsIUBqNEUAQPAGHh0Uc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Wildfire Prediction Dataset (Satellite Images)\n","\n","## Satellite images of areas that previously experienced wildfires in Canada\n","\n","### https://www.kaggle.com/datasets/abdelghaniaaba/wildfire-prediction-dataset"],"metadata":{"id":"ppCH4-j3TFIw"}},{"cell_type":"markdown","source":["## About Dataset\n","\n","### Source\n","\n","Refer to Canada's website for the original wildfires data:\n","\n","*   Forest Fires - Open Government Portal\n","*   Original license for the data:\n","*   Creative Commons 4.0 Attribution (CC-BY) license – Quebec\n","\n","### Dataset\n","\n","This dataset contains satellite images (350x350px) in 2 classes:\n","\n","*   Wildfire : 22710 images\n","*   No wildfire : 20140 images\n","\n","The data was divided into train, test and validation with these percentages :\n","\n","*   Train : ~70%\n","*   Test : ~15%\n","*   Validation : ~15%\n","\n","\n","### How\n","Using Longitude and Latitude coordinates for each wildfire spot (> 0.01 acres burned) found on the dataset above we extracted satellite images of those areas using MapBox API to create a more convenient format of the dataset for deep learning and building a model that can predict whether an area is at risk of a wildfire or not"],"metadata":{"id":"5PApjpHHTNRQ"}},{"cell_type":"markdown","source":["### For handling files within my Google Drive space"],"metadata":{"id":"-o62ZlQkWpGT"}},{"cell_type":"code","source":["import os"],"metadata":{"id":"fleYL6CSWlpb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the current working directory\n","current_dir = os.getcwd()\n","\n","# Print the current working directory\n","current_dir"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"VEbBuB0KWn1c","executionInfo":{"status":"ok","timestamp":1705950200052,"user_tz":-60,"elapsed":6,"user":{"displayName":"Julián Esteban Boronat","userId":"11280705151775376844"}},"outputId":"98d0ec62-7f91-4788-ae2c-8915fe5bcd90"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["# Get the path to the directory\n","initial_path = \"/content/drive/MyDrive/_UPC/Postgraduate in Artificial Intelligence with Deep Learning/X-Final Project/Satellite Images/Wildfire Prediction Dataset (Satellite Images)\"\n","dataset_path = \"/dataset/wildfire_prediction\"\n","\n","# List the contents of the directory\n","contents = os.listdir(initial_path + dataset_path)\n","\n","# Print the contents of the directory\n","for item in contents:\n","  print(item)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r36cK9JSXFXk","executionInfo":{"status":"ok","timestamp":1705950210481,"user_tz":-60,"elapsed":10432,"user":{"displayName":"Julián Esteban Boronat","userId":"11280705151775376844"}},"outputId":"e2b031dc-5f10-4bcd-9da3-b7bc4b9302ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[".DS_Store\n","valid\n","train\n","test\n"]}]},{"cell_type":"markdown","source":["### Importing libraries"],"metadata":{"id":"-WM_rXrRV8m1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-upcI8JpS6OR"},"outputs":[],"source":["import time\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","from typing import Tuple, Dict, Any, List\n","\n","import matplotlib\n","%matplotlib inline\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["### Working with GPU if possible"],"metadata":{"id":"pOKUDeaTV_zz"}},{"cell_type":"code","source":["seed = 123\n","np.random.seed(seed)\n","_ = torch.manual_seed(seed)\n","_ = torch.cuda.manual_seed(seed)"],"metadata":{"id":"h7ZK9W4kVyiF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# we select to work on GPU if it is available in the machine, otherwise will run on CPU\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"],"metadata":{"id":"FXRpsXrYV0bu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Defining the Hyper-parameters"],"metadata":{"id":"_Hp2MyjqV6qX"}},{"cell_type":"code","source":["# Let's define some hyper-parameters\n","hparams = {\n","    'batch_size': 256,\n","    'num_epochs': 10,\n","    'test_batch_size': 256,\n","    'learning_rate': 1e-3,\n","    'weight_decay': 1e-5,\n","    'log_interval': 100,\n","}"],"metadata":{"id":"hTe2URwUV13G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Defining Dataset and DataLoader\n"],"metadata":{"id":"LSukwi0gYwfK"}},{"cell_type":"markdown","source":["#### Gathering Data"],"metadata":{"id":"qlIFJMAYaBfm"}},{"cell_type":"code","source":["initial_path"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"wUsj1PZyaSvA","executionInfo":{"status":"ok","timestamp":1705950215807,"user_tz":-60,"elapsed":7,"user":{"displayName":"Julián Esteban Boronat","userId":"11280705151775376844"}},"outputId":"79782b9f-bf3a-4be5-dfbe-221c40ad9957"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/_UPC/Postgraduate in Artificial Intelligence with Deep Learning/X-Final Project/Satellite Images/Wildfire Prediction Dataset (Satellite Images)'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["dataset_path"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"iCswsOuMbJlQ","executionInfo":{"status":"ok","timestamp":1705950215807,"user_tz":-60,"elapsed":6,"user":{"displayName":"Julián Esteban Boronat","userId":"11280705151775376844"}},"outputId":"1e2405c5-af02-4bd6-df5b-9e002f60d532"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/dataset/wildfire_prediction'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["contents"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6tflSV7QbgBu","executionInfo":{"status":"ok","timestamp":1705950215807,"user_tz":-60,"elapsed":5,"user":{"displayName":"Julián Esteban Boronat","userId":"11280705151775376844"}},"outputId":"3e6b4849-310b-4406-d1b5-4dec117aff38"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['.DS_Store', 'valid', 'train', 'test']"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# defining paths of train, validation and test data\n","train_path = initial_path + dataset_path + \"/train\"\n","test_path = initial_path + dataset_path + \"/test\"\n","valid_path = initial_path + dataset_path + \"/valid\"\n","\n","print(\"Training data: \" + train_path)\n","print(\"Test data: \" + test_path)\n","print(\"Validation data: \" + valid_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h87Czf-ZYz_m","executionInfo":{"status":"ok","timestamp":1705950215807,"user_tz":-60,"elapsed":4,"user":{"displayName":"Julián Esteban Boronat","userId":"11280705151775376844"}},"outputId":"69906b7b-958f-4c8f-91e9-dde4c9ce4f7c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training data: /content/drive/MyDrive/_UPC/Postgraduate in Artificial Intelligence with Deep Learning/X-Final Project/Satellite Images/Wildfire Prediction Dataset (Satellite Images)/dataset/wildfire_prediction/train\n","Test data: /content/drive/MyDrive/_UPC/Postgraduate in Artificial Intelligence with Deep Learning/X-Final Project/Satellite Images/Wildfire Prediction Dataset (Satellite Images)/dataset/wildfire_prediction/test\n","Validation data: /content/drive/MyDrive/_UPC/Postgraduate in Artificial Intelligence with Deep Learning/X-Final Project/Satellite Images/Wildfire Prediction Dataset (Satellite Images)/dataset/wildfire_prediction/valid\n"]}]},{"cell_type":"code","source":["# defining image transformations\n","image_transforms = transforms.Compose([\n","    transforms.Resize((350, 350)),\n","    transforms.ToTensor()\n","])"],"metadata":{"id":"iEFL5HPjbumG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# loading training data using DataLoader\n","train_data = ImageFolder(train_path, transform=image_transforms)\n","train_loader = torch.utils.data.DataLoader(train_data, batch_size=hparams['batch_size'], shuffle=True)"],"metadata":{"id":"43C32DM-JXmc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# loading test data using DataLoader\n","test_data = ImageFolder(test_path, transform=image_transforms)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size=hparams['batch_size'], shuffle=False)\n","\n","# loading validation data using DataLoader\n","val_data = ImageFolder(valid_path, transform=image_transforms)\n","val_loader = torch.utils.data.DataLoader(val_data, batch_size=hparams['batch_size'], shuffle=False)"],"metadata":{"id":"32ZKF6yCcllB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Retrieve a sample from the dataset by simply indexing it\n","img, label = train_data[0]\n","print('Img shape: ', img.shape)\n","print('Label: ', label)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uzQ-waPEjtd5","executionInfo":{"status":"ok","timestamp":1705950346399,"user_tz":-60,"elapsed":117007,"user":{"displayName":"Julián Esteban Boronat","userId":"11280705151775376844"}},"outputId":"6aaabac7-2222-475e-ab94-f0ae6daeb409"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Img shape:  torch.Size([3, 350, 350])\n","Label:  0\n"]}]},{"cell_type":"code","source":["# Sample a BATCH from the dataloader by running over its iterator\n","iter_ = iter(train_loader)\n","bimg, blabel = next(iter_)\n","print('Batch Img shape: ', bimg.shape)\n","print('Batch Label shape: ', blabel.shape)\n","print('Batch Img shape: ', bimg.shape)\n","print('Batch Label shape: ', blabel.shape)\n","print(f'The Batched tensors return a collection of {bimg.shape[0]} images \\\n","({bimg.shape[1]} channel, {bimg.shape[2]} height pixels, {bimg.shape[3]} width \\\n","pixels)')\n","print(f'In the case of the labels, we obtain {blabel.shape[0]} batched integers, one per image')"],"metadata":{"id":"gcaiBaFnjy3s","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705950621050,"user_tz":-60,"elapsed":274655,"user":{"displayName":"Julián Esteban Boronat","userId":"11280705151775376844"}},"outputId":"3e5da2ad-67d6-422f-b898-3e400ea5e3da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Batch Img shape:  torch.Size([256, 3, 350, 350])\n","Batch Label shape:  torch.Size([256])\n","Batch Img shape:  torch.Size([256, 3, 350, 350])\n","Batch Label shape:  torch.Size([256])\n","The Batched tensors return a collection of 256 images (3 channel, 350 height pixels, 350 width pixels)\n","In the case of the labels, we obtain 256 batched integers, one per image\n"]}]},{"cell_type":"code","source":["# Get the class idx + names\n","class_names = train_data.class_to_idx\n","class_names"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jANNDeavLQWa","executionInfo":{"status":"ok","timestamp":1705950621050,"user_tz":-60,"elapsed":9,"user":{"displayName":"Julián Esteban Boronat","userId":"11280705151775376844"}},"outputId":"5c750bf6-c149-4de8-e297-d02b6a90b442"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'nowildfire': 0, 'wildfire': 1}"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["print(\"Train:\")\n","print(f\"Found {len(train_data)} images belonging to {train_data.classes} classes.\")\n","print(\"Test:\")\n","print(f\"Found {len(test_data)} images belonging to {test_data.classes} classes.\")\n","print(\"Val:\")\n","print(f\"Found {len(val_data)} images belonging to {val_data.classes} classes.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"syYEzDuPO-U5","executionInfo":{"status":"ok","timestamp":1705950621050,"user_tz":-60,"elapsed":7,"user":{"displayName":"Julián Esteban Boronat","userId":"11280705151775376844"}},"outputId":"cb0a7a34-f9a5-4570-a160-03ac58d3ab8c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train:\n","Found 30250 images belonging to ['nowildfire', 'wildfire'] classes.\n","Test:\n","Found 6300 images belonging to ['nowildfire', 'wildfire'] classes.\n","Val:\n","Found 6300 images belonging to ['nowildfire', 'wildfire'] classes.\n"]}]},{"cell_type":"markdown","source":["### Trying to feed a CNN"],"metadata":{"id":"Kl59TEFJNvcn"}},{"cell_type":"markdown","source":["### Defining Convolutional Block\n","\n","Creating the `ConvBlock` class to properly do: `Conv2d`, `ReLU`, and `MaxPool2d`. For an input of size `3x350x350`."],"metadata":{"id":"fdJrOPI6QJXb"}},{"cell_type":"code","source":["class ConvBlock(nn.Module):\n","\n","    def __init__(\n","              self,\n","              num_inp_channels: int,\n","              num_out_fmaps: int,\n","              kernel_size: int,\n","              pool_size: int=2\n","            ) -> None:\n","\n","        super().__init__()\n","\n","        # In a convolutional neural network (CNN), the bias term in a convolutional layer represents\n","        # the initial offset applied to the output of the convolution operation. When the bias term is set to False,\n","        # it means that the initial offset is not added to the output.\n","        self.conv = nn.Conv2d(in_channels=num_inp_channels,\n","                              out_channels=num_out_fmaps,\n","                              kernel_size=(kernel_size,kernel_size),\n","                              bias=False)\n","        # Setting inplace=True for the ReLU activation function in the ConvBlock class\n","        # can improve performance by reducing memory usage and computational overhead.\n","        # When inplace=True is set, the activation function modifies the input tensor directly\n","        # instead of creating a new tensor. This can significantly improve performance for large inputs,\n","        # as it eliminates the need to allocate and copy memory for the output tensor.\n","        self.relu = nn.ReLU(inplace=True)\n","        # MaxPool2d is recommended for memory saving in CNNs as it reduces the spatial dimensions of the feature maps,\n","        # effectively reducing the number of parameters and computations, thereby minimizing memory consumption\n","        # and enabling the handling of larger input images, deeper architectures, and more efficient training and inference.\n","        self.maxpool = nn.MaxPool2d(kernel_size=(pool_size,pool_size))\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        return self.maxpool(self.relu(self.conv(x)))"],"metadata":{"id":"62yzTFXIL9WB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"0vbjsEneaBCq"}},{"cell_type":"code","source":["model = ConvBlock(\n","    num_inp_channels=3,\n","    num_out_fmaps=8,\n","    kernel_size=2)\n","\n","# run forward pass\n","x = torch.randn(1, 3, 350, 350)\n","y = model(x)\n","\n","print(f'Input shape: {x.shape}')\n","print(f'ConvBlock output shape: {y.shape}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XIQXHMFGRDxC","executionInfo":{"status":"ok","timestamp":1705950621050,"user_tz":-60,"elapsed":5,"user":{"displayName":"Julián Esteban Boronat","userId":"11280705151775376844"}},"outputId":"bcdab4f1-53bd-4208-b4d6-82262b19223a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input shape: torch.Size([1, 3, 350, 350])\n","ConvBlock output shape: torch.Size([1, 8, 174, 174])\n"]}]},{"cell_type":"markdown","source":["### Importing Softmax activation function for the Binary Classification"],"metadata":{"id":"0OwjyqkYdkmL"}},{"cell_type":"code","source":["from torch.nn.modules.activation import Softmax"],"metadata":{"id":"Ilra4Sk0YYJL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## CNNBinaryClassifier Net"],"metadata":{"id":"98LXrQZ9eHEF"}},{"cell_type":"code","source":["class CNNBinaryClassifier(nn.Module):\n","\n","    def __init__(self) -> None:\n","        super().__init__()\n","\n","        self.conv1 = ConvBlock(num_inp_channels=3, num_out_fmaps=8, kernel_size=2)\n","        self.conv2 = ConvBlock(num_inp_channels=8, num_out_fmaps=16, kernel_size=2)\n","        self.conv3 = ConvBlock(num_inp_channels=16, num_out_fmaps=32, kernel_size=2)\n","\n","        # Defining the fully connected layers\n","        self.mlp = nn.Sequential(\n","            nn.Dropout(0.4),\n","            nn.Linear(in_features=56448, out_features=2048),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(in_features=2048, out_features=300),\n","            nn.ReLU(),\n","            nn.Dropout(0.5),\n","            nn.Linear(in_features=300, out_features=2),\n","            nn.ReLU(),\n","            nn.Softmax(dim=1)\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","        bsz, nch, height, width = x.shape\n","        x = torch.reshape(x, (bsz, (nch * height * width)))\n","        # print(x.shape) = torch.Size([1, 56448])\n","        y = self.mlp(x)\n","        return y"],"metadata":{"id":"GUoo3AeFYT4i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = CNNBinaryClassifier()\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E2Fv9ZYumHjD","executionInfo":{"status":"ok","timestamp":1705950622357,"user_tz":-60,"elapsed":1310,"user":{"displayName":"Julián Esteban Boronat","userId":"11280705151775376844"}},"outputId":"2731c6da-ef67-447c-c8c6-013d01fca10f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CNNBinaryClassifier(\n","  (conv1): ConvBlock(\n","    (conv): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), bias=False)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (conv2): ConvBlock(\n","    (conv): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), bias=False)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (conv3): ConvBlock(\n","    (conv): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), bias=False)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (mlp): Sequential(\n","    (0): Dropout(p=0.4, inplace=False)\n","    (1): Linear(in_features=56448, out_features=2048, bias=True)\n","    (2): ReLU()\n","    (3): Dropout(p=0.5, inplace=False)\n","    (4): Linear(in_features=2048, out_features=300, bias=True)\n","    (5): ReLU()\n","    (6): Dropout(p=0.5, inplace=False)\n","    (7): Linear(in_features=300, out_features=2, bias=True)\n","    (8): ReLU()\n","    (9): Softmax(dim=1)\n","  )\n",")"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["# Forward a toy example emulating the image size\n","bnet_classifier = CNNBinaryClassifier()\n","y = bnet_classifier(torch.randn(1, 3, 350, 350)).to(device)\n","print(f\"Output shape: {y.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lslrFVABecHA","executionInfo":{"status":"ok","timestamp":1705950623742,"user_tz":-60,"elapsed":1387,"user":{"displayName":"Julián Esteban Boronat","userId":"11280705151775376844"}},"outputId":"116fc2d6-4efe-41f3-dbde-daad21ae2742"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output shape: torch.Size([1, 2])\n"]}]},{"cell_type":"markdown","source":["## Now it's time to train and test the model."],"metadata":{"id":"-k0nfaW4oVhc"}},{"cell_type":"markdown","source":["### compute_accuracy"],"metadata":{"id":"_EOjpDKfptyi"}},{"cell_type":"code","source":["def compute_accuracy(predicted_batch: torch.Tensor, label_batch: torch.Tensor) -> float:\n","    \"\"\"\n","    Define the Accuracy metric in the function below by:\n","      (1) obtain the maximum for each predicted element in the batch to get the\n","        class (it is the maximum index of the num_classes array per batch sample)\n","        (look at torch.argmax in the PyTorch documentation)\n","      (2) compare the predicted class index with the index in its corresponding\n","        neighbor within label_batch\n","      (3) sum up the number of affirmative comparisons and return the summation\n","\n","    Parameters:\n","    -----------\n","    predicted_batch: torch.Tensor shape: [BATCH_SIZE, N_CLASSES]\n","        Batch of predictions\n","    label_batch: torch.Tensor shape: [BATCH_SIZE, 1]\n","        Batch of labels / ground truths.\n","    \"\"\"\n","    pred = predicted_batch.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n","    acum = pred.eq(label_batch.view_as(pred)).sum().item()\n","    return acum"],"metadata":{"id":"ZxXyxeMnoFnh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### train_epoch"],"metadata":{"id":"qT9cXA-Op38I"}},{"cell_type":"code","source":["def train_epoch(\n","        train_loader: torch.utils.data.DataLoader,\n","        network: torch.nn.Module,\n","        optimizer: torch.optim,\n","        criterion: torch.nn.functional,\n","        log_interval: int,\n","        ) -> Tuple[float, float]:\n","\n","    # Activate the train=True flag inside the model\n","    network.train()\n","\n","    train_loss = []\n","    acc = 0.\n","    avg_weight = 0.1\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","\n","        # Move input data and labels to the device\n","        data, target = data.to(device), target.to(device)\n","\n","        # Set network gradients to 0.\n","        optimizer.zero_grad()\n","\n","        # Forward batch of images through the network\n","        output = network(data)\n","\n","        # Compute loss\n","        loss = criterion(output, target)\n","\n","        # Compute backpropagation\n","        loss.backward()\n","\n","        # Update parameters of the network\n","        optimizer.step()\n","\n","        # Compute metrics\n","        acc += compute_accuracy(output, target)\n","        train_loss.append(loss.item())\n","\n","        if batch_idx % log_interval == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))\n","    avg_acc = 100. * acc / len(train_loader.dataset)\n","\n","    return np.mean(train_loss), avg_acc"],"metadata":{"id":"qnJo4Qu_p3aR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### test_epoch"],"metadata":{"id":"9j5c2-KHqZzA"}},{"cell_type":"code","source":["@torch.no_grad() # decorator: avoid computing gradients\n","def test_epoch(\n","        test_loader: torch.utils.data.DataLoader,\n","        network: torch.nn.Module,\n","        ) -> Tuple[float, float]:\n","\n","    # Dectivate the train=True flag inside the model\n","    network.eval()\n","\n","    test_loss = []\n","    acc = 0\n","    for data, target in test_loader:\n","        data, target = data.to(device), target.to(device)\n","\n","        output = network(data)\n","\n","        # Apply the loss criterion and accumulate the loss\n","        test_loss.append(criterion(output, target).item())\n","\n","        # compute number of correct predictions in the batch\n","        acc += compute_accuracy(output, target)\n","\n","    # Average accuracy across all correct predictions batches now\n","    test_acc = 100. * acc / len(test_loader.dataset)\n","    test_loss = np.mean(test_loss)\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, acc, len(test_loader.dataset), test_acc,\n","        ))\n","    return test_loss, test_acc"],"metadata":{"id":"L7E2LmzOqbi0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_losses = []\n","test_losses = []\n","train_accs = []\n","test_accs = []\n","network = CNNBinaryClassifier()\n","network.to(device)\n","\n","optimizer = torch.optim.Adam(network.parameters(), lr=hparams['learning_rate'], weight_decay=hparams['weight_decay'])\n","criterion = nn.CrossEntropyLoss().to(device)\n","\n","for epoch in range(hparams['num_epochs']):\n","\n","    # Compute & save the average training loss for the current epoch\n","    train_loss, train_acc = train_epoch(train_loader, network, optimizer, criterion, hparams[\"log_interval\"])\n","    train_losses.append(train_loss)\n","    train_accs.append(train_acc)\n","\n","    # Compute & save the average test loss & accuracy for the current epoch\n","    test_loss, test_accuracy = test_epoch(test_loader, network)\n","\n","    test_losses.append(test_loss)\n","    test_accs.append(test_accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"kjzDDJm-qr6s","executionInfo":{"status":"error","timestamp":1705954370356,"user_tz":-60,"elapsed":3746616,"user":{"displayName":"Julián Esteban Boronat","userId":"11280705151775376844"}},"outputId":"fb630023-dd74-41ea-bed8-70b6fb5784b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 0 [0/30250 (0%)]\tLoss: 0.693120\n"]},{"output_type":"error","ename":"OSError","evalue":"image file is truncated (16 bytes not processed)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-7d69e6511140>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Compute & save the average training loss for the current epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"log_interval\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtrain_accs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-26-2ee2abde280f>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(train_loader, network, optimizer, criterion, log_interval)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mavg_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Move input data and labels to the device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \"\"\"\n\u001b[1;32m    228\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    935\u001b[0m         \"\"\"\n\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transparency\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m                                         \u001b[0;34mf\"({len(b)} bytes not processed)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                                     )\n\u001b[0;32m--> 266\u001b[0;31m                                     \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: image file is truncated (16 bytes not processed)"]}]}]}